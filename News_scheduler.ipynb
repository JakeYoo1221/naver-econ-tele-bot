{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155f361c-a1f9-403f-ad08-04e38916bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1dc72-43dd-45f3-9a2d-18432cd18c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import schedule\n",
    "import pytz\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "# =========================\n",
    "# ğŸ”§ ì„¤ì •ê°’ (ì—¬ê¸°ë§Œ ìˆ˜ì •)\n",
    "# =========================\n",
    "TELEGRAM_TOKEN = \"7636841205:AAFScTMxkiNxg9AtGufSOUHS6UTrbwE46tQ\"   # BotFather ë°œê¸‰ í† í°\n",
    "CHAT_ID       = \"7246158575\"       # @userinfobot ë˜ëŠ” getUpdatesë¡œ í™•ì¸í•œ ìˆ«ì ID\n",
    "SEND_TIME     = \"07:50\"                # ë§¤ì¼ ì „ì†¡ ì‹œê° (í•œêµ­ì‹œê°„)\n",
    "NEWS_LIMIT    = 5                      # ê¸°ì‚¬ ê°œìˆ˜\n",
    "SUMMARY_MAXLEN= 200                    # ìš”ì•½ ìµœëŒ€ í† í° ê¸¸ì´ (ì§§ê²Œ/ë¹ ë¥´ê²Œ ì›í•˜ë©´ ì¤„ì´ì„¸ìš”)\n",
    "\n",
    "# =========================\n",
    "# ê³µí†µ ìƒìˆ˜/í—¤ë”\n",
    "# =========================\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# í…”ë ˆê·¸ë¨ ë„ìš°ë¯¸ (ìë™ ë¶„í•  ì „ì†¡)\n",
    "# =========================\n",
    "def send_telegram_message(text: str):\n",
    "    \"\"\"\n",
    "    í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ(ì•½ 4096ì) ëŒ€ë¹„ ìë™ ë¶„í•  ì „ì†¡.\n",
    "    \"\"\"\n",
    "    max_len = 3800  # ì•ˆì „ ë§ˆì§„\n",
    "    url = f\"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage\"\n",
    "    if len(text) <= max_len:\n",
    "        payload = {\"chat_id\": CHAT_ID, \"text\": text}\n",
    "        return requests.post(url, data=payload, timeout=15).json()\n",
    "\n",
    "    # ê¸¸ë©´ ë¶„í• \n",
    "    parts = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_len, len(text))\n",
    "        parts.append(text[start:end])\n",
    "        start = end\n",
    "\n",
    "    last_resp = None\n",
    "    for p in parts:\n",
    "        payload = {\"chat_id\": CHAT_ID, \"text\": p}\n",
    "        last_resp = requests.post(url, data=payload, timeout=15).json()\n",
    "        time.sleep(0.5)  # ê³¼ë„ í˜¸ì¶œ ë°©ì§€\n",
    "    return last_resp\n",
    "\n",
    "# =========================\n",
    "# ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤: í—¤ë“œë¼ì¸ ìˆ˜ì§‘ (ë‹¤ì¤‘ ì„ íƒì + í´ë°±)\n",
    "# =========================\n",
    "def _extract_articles(soup):\n",
    "    \"\"\"\n",
    "    ë„¤ì´ë²„ ì„¹ì…˜ êµ¬ì¡°ê°€ ë°”ë€Œì–´ë„ ê²¬ë”œ ìˆ˜ ìˆê²Œ ë‹¤ì¤‘ ì„ íƒì ì‚¬ìš©.\n",
    "    ë°˜í™˜: [(title, href), ...]\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "\n",
    "    # êµ¬í˜• ì„¹ì…˜ UI\n",
    "    for a in soup.select(\"div.cluster_body a.cluster_text_headline\"):\n",
    "        t, h = a.get_text(strip=True), a.get(\"href\", \"\")\n",
    "        if t and h:\n",
    "            candidates.append((t, h))\n",
    "\n",
    "    # ì‹ í˜• ì„¹ì…˜ UI (/section/101)\n",
    "    for a in soup.select(\"div.sa_text a.sa_text_title\"):\n",
    "        t, h = a.get_text(strip=True), a.get(\"href\", \"\")\n",
    "        if t and h:\n",
    "            candidates.append((t, h))\n",
    "\n",
    "    # ê¸°íƒ€ ë³€í˜•\n",
    "    for a in soup.select(\"a.sh_text_headline, a.cluster_head_link, a.sa_item_title\"):\n",
    "        t, h = a.get_text(strip=True), a.get(\"href\", \"\")\n",
    "        if t and h:\n",
    "            candidates.append((t, h))\n",
    "\n",
    "    # ëª¨ë°”ì¼ ì„¹ì…˜ í´ë°±\n",
    "    for a in soup.select(\"div.press_story a, div.section_list a\"):\n",
    "        t, h = a.get_text(strip=True), a.get(\"href\", \"\")\n",
    "        if t and h and \"news.naver.com\" in h:\n",
    "            candidates.append((t, h))\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±°\n",
    "    seen, uniq = set(), []\n",
    "    for t, h in candidates:\n",
    "        key = (t, h)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append((t, h))\n",
    "    return uniq\n",
    "\n",
    "def fetch_economy_headlines(limit=NEWS_LIMIT):\n",
    "    urls = [\n",
    "        \"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=101\",\n",
    "        \"https://news.naver.com/section/101\",\n",
    "        \"https://m.news.naver.com/section/101\",\n",
    "    ]\n",
    "    for url in urls:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            arts = _extract_articles(soup)\n",
    "            if arts:\n",
    "                return arts[:limit]\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # ìµœí›„ í´ë°±: RSS\n",
    "    try:\n",
    "        rss = requests.get(\"https://news.naver.com/rss/section/101.xml\", headers=HEADERS, timeout=10)\n",
    "        rss.raise_for_status()\n",
    "        soup = BeautifulSoup(rss.text, \"xml\")\n",
    "        items = []\n",
    "        for it in soup.select(\"item\")[:limit]:\n",
    "            title = it.title.get_text(strip=True)\n",
    "            link = it.link.get_text(strip=True)\n",
    "            items.append((title, link))\n",
    "        if items:\n",
    "            return items\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return []\n",
    "\n",
    "# =========================\n",
    "# ë³¸ë¬¸ í¬ë¡¤ë§ (ë‹¤ì¤‘ ì„ íƒì)\n",
    "# =========================\n",
    "def fetch_article_text(url, max_chars=800):\n",
    "    \"\"\"\n",
    "    ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë‹¤ì–‘í•œ ì„ íƒìë¡œ ì‹œë„. ê¸¸ì´ëŠ” ì²˜ë¦¬ì†ë„ ìœ„í•´ ì»·.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        body_selectors = [\n",
    "            \"div#dic_area\",           # í‘œì¤€(ë°ìŠ¤í¬í†±)\n",
    "            \"div#newsct_article\",     # ì‹ í˜• í…œí”Œë¦¿\n",
    "            \"div.article_body\",       # í´ë°±\n",
    "            \"div#articeBody\",         # ê³¼ê±° ëŒ€ì²´\n",
    "        ]\n",
    "        text = \"\"\n",
    "        for sel in body_selectors:\n",
    "            node = soup.select_one(sel)\n",
    "            if node:\n",
    "                text = node.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    break\n",
    "        return text[:max_chars] if text else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# =========================\n",
    "# KoBART ìš”ì•½ (ìµœì´ˆ 1íšŒ ë¡œë”©)\n",
    "# =========================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"ğŸ”„ KoBART ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"gogamza/kobart-summarization\")\n",
    "MODEL = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-summarization\").to(DEVICE)\n",
    "print(\"âœ… KoBART ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "\n",
    "def kobart_summarize(text: str, max_len=SUMMARY_MAXLEN) -> str:\n",
    "    if not text or len(text) < 50:\n",
    "        return \"ë³¸ë¬¸ì´ ì§§ì•„ ìš”ì•½ ë¶ˆê°€\"\n",
    "    try:\n",
    "        inputs = TOKENIZER([text], max_length=1024, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        summary_ids = MODEL.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            num_beams=2,            # ì†ë„ ìµœì í™” (í’ˆì§ˆâ†‘ë©´ 3~4)\n",
    "            max_length=max_len,     # ìš”ì•½ ê¸¸ì´\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return TOKENIZER.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        return f\"(ìš”ì•½ ì‹¤íŒ¨: {e})\"\n",
    "\n",
    "# =========================\n",
    "# ìµœì¢… ìš”ì•½ ë©”ì‹œì§€ ìƒì„±\n",
    "# =========================\n",
    "def build_news_message(limit=NEWS_LIMIT) -> str:\n",
    "    lines = []\n",
    "    kst = pytz.timezone(\"Asia/Seoul\")\n",
    "    now_str = datetime.now(kst).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    lines.append(f\"â° {now_str} ê¸°ì¤€\\n\")\n",
    "    lines.append(\"ğŸ“Š ì˜¤ëŠ˜ì˜ ê²½ì œ ë‰´ìŠ¤ ìš”ì•½ (ë„¤ì´ë²„)\\n\")\n",
    "\n",
    "    articles = fetch_economy_headlines(limit=limit)\n",
    "    if not articles:\n",
    "        lines.append(\"âš ï¸ ìµœì‹  ê¸°ì‚¬ ëª©ë¡ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í• ê²Œìš”.\")\n",
    "        return \"\\n\".join(lines).strip()\n",
    "\n",
    "    for i, (title, link) in enumerate(articles, 1):\n",
    "        body = fetch_article_text(link, max_chars=800)\n",
    "        summary = kobart_summarize(body, max_len=SUMMARY_MAXLEN)\n",
    "        lines.append(f\"{i}) {title}\\nğŸ”— {link}\\nğŸ“ {summary}\\n\")\n",
    "\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "# =========================\n",
    "# ìŠ¤ì¼€ì¤„ ì‘ì—…\n",
    "# =========================\n",
    "def job():\n",
    "    try:\n",
    "        msg = build_news_message(limit=NEWS_LIMIT)\n",
    "        resp = send_telegram_message(msg)\n",
    "        print(\"í…”ë ˆê·¸ë¨ ì „ì†¡ ê²°ê³¼:\", resp)\n",
    "    except Exception as e:\n",
    "        print(\"ì „ì†¡ ì˜¤ë¥˜:\", e)\n",
    "\n",
    "# ë§¤ì¼ KST ê¸°ì¤€ SEND_TIMEì— ì‹¤í–‰\n",
    "schedule.every().day.at(SEND_TIME).do(job)\n",
    "print(f\"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨: ë§¤ì¼ {SEND_TIME} (KST)ì— ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ {NEWS_LIMIT}ê°œ KoBART ìš”ì•½ ì „ì†¡\")\n",
    "\n",
    "# ì¦‰ì‹œ í•œ ë²ˆ í…ŒìŠ¤íŠ¸ ì „ì†¡í•˜ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ í•´ì œ:\n",
    "# job()\n",
    "\n",
    "# ë£¨í”„\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769e3d0-a6e1-43b0-82c8-41fe6ae04690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
